[【多模态沉浸式教学】小白、散养研究生看完直接给自己放暑假！！--人工智能/多模态_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1vw4m1a76k/?spm_id_from=333.788&vd_source=9741da01eb5597a19fd0610df71471cc)

### 国内外gpt工具：
[通义tongyi.ai_你的全能AI助手-通义千问](https://tongyi.aliyun.com/qianwen/?spm=5176.28326591.0.0.40f76ee1zBYunq&sessionId=ab7f2bf3cef24d1896c00980113142a8)

[文心一言](https://yiyan.baidu.com/)

[智谱清言](https://chatglm.cn/main/alltoolsdetail?lang=zh)

[零一万物-AI2.0大模型技术和应用的全球公司](https://www.lingyiwanwu.com/)

[天工AI助手 — 双千亿级大语言模型](https://sso.tiangong.cn/?redirect=https%3A%2F%2Fwork.tiangong.cn%2Fchatdoc%2Fd%2Fdoc%2Findex&client_id=200006)

[Kimi.ai - 帮你看更大的世界](https://kimi.moonshot.cn/)

[秘塔AI搜索](https://metaso.cn/)

[Chatgpt](https://chatgpt.com/c/e59583ca-2ef8-4c23-907f-23e4b5eba5ec)

### 国内大模型api开放平台
[智谱AI开放平台](https://open.bigmodel.cn/)

[零一万物大模型开放平台](https://platform.lingyiwanwu.com/apikeys)

[百度智能云千帆大模型平台](https://console.bce.baidu.com/qianfan/ais/console/applicationConsole/application/v1)

[阿里云登录 - 欢迎登录阿里云，安全稳定的云计算服务平台](https://dashscope.console.aliyun.com/apiKey)



### PS：常用命令参考
```plain
旧版API调用：
pip install openai==0.28 -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install openai -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install openai-python -i https://pypi.tuna.tsinghua.edu.cn/simple

查看文档 https://github.com/openai/openai-python/discussions/742 
https://github.com/openai/openai-python

通义千问：
api_key :  xxx
pip install dashscope -i https://pypi.tuna.tsinghua.edu.cn/simple

阿里云RAM 访问控制：xxx
用户登录名称 xxx
AccessKey ID  ：xxx
AccessKey Secret ： xxx
登录密码： xxx

智谱清言chatgml : api_key: xxx


本地部署大语言模型
https://ollama.com/

默认模型路径： C:\Users\LingJiaXiaoHu\.ollama\models
修改环境变量 新建OLLAMA_MODELS   键值为新模型保存路径
ollama list 显示本地模型列表
运行
ollama run llama3        8b
ollama run qwen2        7b
ollama run gemma      7b
ollama run phi3           3.8b
视觉模型:   ollama run llava:7b 

ollama run llama3.1    8b

安装docker后
cmd  运行 docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
gpu支持： docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda

```

## 一、python简单调用LLM
阿里云通义千问api_key：[https://dashscope.console.aliyun.com/apiKey](https://dashscope.console.aliyun.com/apiKey)

**以阿里云通义千问大模型为例：**

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Time    : 2024/6/25 21:09
# @Author  : LingJiaXiaoHu
# @File    : qwen.py
# @Software: win11 pytorch(GPU版本） python3.9.16
from openai import OpenAI
import os

'''
# 非流式调用示例
def get_response():
    client = OpenAI(
        #api_key=os.getenv("DASHSCOPE_API_KEY"), # 如果您没有配置环境变量，请在此处用您的API Key进行替换
        api_key="sk-c6fe911336a548f29641bd2348904215",
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",  # 填写DashScope SDK的base_url
    )
    completion = client.chat.completions.create(
        model="qwen-plus",
        messages=[{'role': 'system', 'content': 'You are a helpful assistant.'},
                  {'role': 'user', 'content': '你是谁？'}]
        )
    print(completion.model_dump_json()
'''

import json
# 流式调用示例
def get_response():
    while True:
        user_input = input("\n请输入你的问题(输入q结束提问): ").strip()   # 去除输入前后的空白字符
        # 检查用户是否想退出
        if user_input.lower() == 'q':
            print("感谢使用，再见！")
            break

        client = OpenAI(
            # 如果您没有配置环境变量，请在此处用您的API Key进行替换
            #api_key=os.getenv("DASHSCOPE_API_KEY"),
            api_key="sk-c6fe911336a548f29641bd2348904215",
            # 填写DashScope SDK的base_url
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        )
        completion = client.chat.completions.create(
            model="qwen-plus",
            messages=[{'role': 'system', 'content': 'You are a helpful assistant.'},
                      {'role': 'user', 'content': user_input}],
            stream=True,
            # 通过以下设置，在流式输出的最后一行展示token使用信息
            stream_options={"include_usage": True}
        )

        for chunk in completion:
            json_text = chunk.model_dump_json()
            #print(json_text)

            # 假设 responses 是一个用于存储所有响应内容的列表
            responses = []
            # 解析 JSON 字符串为字典
            response_data = json.loads(json_text)
            # 提取响应内容
            if 'choices' in response_data and response_data['choices']:
                delta = response_data['choices'][0]['delta']
                if 'content' in delta and delta['content'].strip():
                    # 将 content 添加到 responses 列表中
                    responses.append(delta['content'])

            # 可以选择打印出当前响应内容         打印完整的文本
            print(delta['content'] if 'content' in delta else "无响应内容",end='')


if __name__ == '__main__':
    get_response()

```

![](https://cdn.nlark.com/yuque/0/2024/png/39216292/1725264802360-29770a75-b48b-4443-bd24-524e2cd0e96c.png)



```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Time    : 2024/7/1 20:40
# @Author  : LingJiaXiaoHu
# @File    : qwen_vl.py
# @Software: win11 pytorch(GPU版本） python3.9.16
from openai import OpenAI
import os

# VL模型流式调用示例
import json
def get_response():
    while True:
        user_img = input("\n请输入你图片地址(输入q结束提问): ").strip()   # 去除输入前后的空白字符
        # 检查用户是否想退出
        if user_img.lower() == 'q':
            print("感谢使用，再见！")
            break

        client = OpenAI(
            # 如果您没有配置环境变量，请在此处用您的API Key进行替换
            #api_key=os.getenv("DASHSCOPE_API_KEY"),
            api_key="sk-c6fe911336a548f29641bd2348904215",
            # 填写DashScope SDK的base_url
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        )
        completion = client.chat.completions.create(
            model="qwen-vl-plus",
            messages=[
                {
                  "role": "user",
                  "content": [
                    {
                      "type": "text",
                      "text": "这是什么"
                    },
                    {
                      "type": "image_url",
                      "image_url": {
                        #"url": "https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg"
                        #"url": "https://gw.alicdn.com/imgextra/i3/O1CN01Am8Sra21Zaice06ax_!!6000000006999-2-tps-1130-500.png_570x10000.jpg_.webp"
                        #"url": "https://ht2024.oss-cn-wuhan-lr.aliyuncs.com/Saved%20Pictures/%E6%B5%B7%E7%BB%B5%E5%AE%9D%E5%AE%9D01.jpg"
                        "url": user_img
                      }
                    }
                  ]
                }
              ],
            top_p=0.8,
            stream=True,
            # 通过以下设置，在流式输出的最后一行展示token使用信息
            stream_options={"include_usage": True}
            )

        # for chunk in completion:
        #   print(chunk.model_dump_json())

        for chunk in completion:
            json_text = chunk.model_dump_json()
            #print(json_text)

            # 假设 responses 是一个用于存储所有响应内容的列表
            responses = []
            # 解析 JSON 字符串为字典
            response_data = json.loads(json_text)
            # 提取响应内容
            if 'choices' in response_data and response_data['choices']:
                delta = response_data['choices'][0]['delta']
                if 'content' in delta and delta['content'].strip():
                    # 将 content 添加到 responses 列表中
                    responses.append(delta['content'])

            # 可以选择打印出当前响应内容         打印完整的文本
            content = delta['content'] if 'content' in delta else "无响应内容"
            #print(content)
            print(content, end="")



            # chunk_size = 50  # 每次打印的字符数量
            # content = delta.get('content', "")
            # # 使用生成器表达式和join函数分块打印
            # for i in range(0, len(content), chunk_size):
            #     # 或者如果你想要连续输出而不换行，可以使用下面的代码
            #     print(content[i:i+chunk_size], end='')
            #     print()  # 在每块结束时换行



            # 设置每行字符数
            #chars_per_line = 30
            # 计数器初始化
            # counter = 0
            # for char in content:
            #     print(char, end='')  # 输出字符但不换行
            #     counter += 1  # 更新计数器
            #     # if counter % chars_per_line == 0:  # 当达到设定的字符数时换行
            #     if counter == 50:
            #         print()  # 换行

if __name__=='__main__':
    get_response()
```

![](https://cdn.nlark.com/yuque/0/2024/png/39216292/1725264718615-fed41c9f-358a-4eeb-9c16-774d97b0ec24.png)



```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Time    : 2024/7/1 20:17
# @Author  : LingJiaXiaoHu
# @File    : qwen_func.py
# @Software: win11 pytorch(GPU版本） python3.9.16
from openai import OpenAI
from datetime import datetime
import json
import os

client = OpenAI(
    #api_key=os.getenv("DASHSCOPE_API_KEY"),  # 如果您没有配置环境变量，请在此处用您的API Key进行替换
    api_key = "sk-c6fe911336a548f29641bd2348904215",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",  # 填写DashScope SDK的base_url
)

# 定义工具列表，模型在选择使用哪个工具时会参考工具的name和description
tools = [
    # 工具1 获取当前时刻的时间
    {
        "type": "function",
        "function": {
            "name": "get_current_time",
            "description": "当你想知道现在的时间时非常有用。",
            "parameters": {}  # 因为获取当前时间无需输入参数，因此parameters为空字典
        }
    },
    # 工具2 获取指定城市的天气
    {
        "type": "function",
        "function": {
            "name": "get_current_weather",
            "description": "当你想查询指定城市的天气时非常有用。",
            "parameters": {  # 查询天气时需要提供位置，因此参数设置为location
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "城市或县区，比如北京市、杭州市、余杭区等。"
                    }
                }
            },
            "required": [
                "location"
            ]
        }
    }
]


# 模拟天气查询工具。返回结果示例：“北京今天是晴天。”
def get_current_weather(location):
    return f"{location}今天是雨天。 "


# 查询当前时间的工具。返回结果示例：“当前时间：2024-04-15 17:15:18。“
def get_current_time():
    # 获取当前日期和时间
    current_datetime = datetime.now()
    # 格式化当前日期和时间
    formatted_time = current_datetime.strftime('%Y-%m-%d %H:%M:%S')
    # 返回格式化后的当前时间
    return f"当前时间：{formatted_time}。"


# 封装模型响应函数
def get_response(messages):
    completion = client.chat.completions.create(
        model="qwen-max",
        messages=messages,
        tools=tools
    )
    return completion.model_dump()


def call_with_messages():
    print('\n')
    messages = [
        {
            "content": input('请输入：'),  # 提问示例："现在几点了？" "一个小时后几点" "北京天气如何？"
            "role": "user"
        }
    ]
    print("-" * 60)
    # 模型的第一轮调用
    i = 1
    first_response = get_response(messages)
    assistant_output = first_response['choices'][0]['message']
    print(f"\n第{i}轮大模型输出信息：{first_response}\n")
    messages.append(assistant_output)
    # 如果不需要调用工具，则直接返回最终答案
    if assistant_output['tool_calls'] == None:  # 如果模型判断无需调用工具，则将assistant的回复直接打印出来，无需进行模型的第二轮调用
        print(f"无需调用工具，我可以直接回复：{assistant_output['content']}")
        return
    # 如果需要调用工具，则进行模型的多轮调用，直到模型判断无需调用工具
    while assistant_output['tool_calls'] != None:
        # 如果判断需要调用查询天气工具，则运行查询天气工具
        if assistant_output['tool_calls'][0]['function']['name'] == 'get_current_weather':
            tool_info = {"name": "get_current_weather", "role": "tool"}
            # 提取位置参数信息
            location = json.loads(assistant_output['tool_calls'][0]['function']['arguments'])['properties']['location']
            tool_info['content'] = get_current_weather(location)
        # 如果判断需要调用查询时间工具，则运行查询时间工具
        elif assistant_output['tool_calls'][0]['function']['name'] == 'get_current_time':
            tool_info = {"name": "get_current_time", "role": "tool"}
            tool_info['content'] = get_current_time()
        print(f"工具输出信息：{tool_info['content']}\n")
        print("-" * 60)
        messages.append(tool_info)
        assistant_output = get_response(messages)['choices'][0]['message']
        messages.append(assistant_output)
        i += 1
        print(f"第{i}轮大模型输出信息：{assistant_output}\n")
    print(f"最终答案：{assistant_output['content']}")


if __name__ == '__main__':
    call_with_messages()
# 杭州和北京天气怎么样？现在几点了？
```

![](https://cdn.nlark.com/yuque/0/2024/png/39216292/1725264892212-427546fd-828c-4346-b8e9-71d7f38a0ec1.png)

## 二、Ollama本地部署LLM
[手把手教你使用Ollama怎么在本地部署AI开源大模型_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV17z421U7N2/?spm_id_from=333.788.top_right_bar_window_custom_collection.content.click&vd_source=9741da01eb5597a19fd0610df71471cc)

[GitHub - open-webui/open-webui: User-friendly WebUI for LLMs (Formerly Ollama WebUI)](https://github.com/open-webui/open-webui)

[Ollama](https://ollama.com/) --- [https://ollama.com/](https://ollama.com/)

[library](https://ollama.com/library) （ollama models）

[Docker: Accelerated Container Application Development](https://www.docker.com/)

```plain
本地部署大语言模型
https://ollama.com/

默认模型路径： C:\Users\LingJiaXiaoHu\.ollama\models
修改环境变量 新建OLLAMA_MODELS   键值为新模型保存路径
ollama list 显示本地模型列表
运行
ollama run llama3        8b
ollama run qwen2        7b
ollama run gemma      7b
ollama run phi3           3.8b
视觉模型:   ollama run llava:7b 

ollama run llama3.1    8b

安装docker后
cmd  运行 docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
gpu支持： docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda

```

### 1.安装ollama
[Ollama](https://ollama.com/) 下载Ollama软件进行安装

![](https://cdn.nlark.com/yuque/0/2024/png/39216292/1725267562943-907ae6dd-f1c5-4b60-8987-33af6343ea1f.png)

**安装好后开始下载模型并尝试运行：**

![](https://cdn.nlark.com/yuque/0/2024/png/39216292/1725267045308-4ca13ccb-e505-463f-9709-53a24b61cb59.png)

![](https://cdn.nlark.com/yuque/0/2024/png/39216292/1725267075306-d993ceb1-32ec-4c33-a6b4-fca4edef4d2a.png)

![](https://cdn.nlark.com/yuque/0/2024/png/39216292/1725267154383-f421f459-d1b5-49da-a79d-1de1da2cad08.png)



### 2.安装docker
下载安装docker：[https://www.docker.com/](https://www.docker.com/)

![](https://cdn.nlark.com/yuque/0/2024/png/39216292/1725266751488-7f2122a9-1a04-4b21-ab5c-82187322d0b9.png)

### 3.联结Open-Webui
open-webui界面安装：[https://github.com/open-webui/open-webui](https://github.com/open-webui/open-webui)

```plain
安装docker后
cmd  运行 docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
gpu支持： docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda

```

![](https://cdn.nlark.com/yuque/0/2024/png/39216292/1725267001794-18f88458-4d2d-44b5-a0ad-8cb1bc580b42.png)

![](https://cdn.nlark.com/yuque/0/2024/png/39216292/1725268369077-1d47029b-2689-414b-9cd2-11ec71b5d26a.png)



**点击端口3000:8080进入webui界面，**

![](https://cdn.nlark.com/yuque/0/2024/png/39216292/1725268612018-2ac75766-619b-48cc-a3bc-dfac285dec25.png)



**可以选择自己满意的大模型使用**

![](https://cdn.nlark.com/yuque/0/2024/png/39216292/1725268677027-e5edfba1-f16b-4622-9bfd-1fead06192a3.png)

### 4.其他
**PS：模型默认安装下载到C盘，可以更改环境变量默认移动到其他盘**

![](https://cdn.nlark.com/yuque/0/2024/png/39216292/1725270977508-b2675f9a-09da-4b84-8471-c949b33db3a4.png)

![](https://cdn.nlark.com/yuque/0/2024/png/39216292/1725270620362-070cc0a0-3142-4395-b0eb-5c3635288386.png)

