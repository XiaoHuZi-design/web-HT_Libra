# 1.urllib基本使用
```python
#_*_coding:utf-8_*_
#@Time:2022/10/1219:13
#@Auther:邻家小胡
#@File:urllib基本使用
#@Project:untitled


import urllib.request

#（1）定义一个url   就是你要访问的地址
url='http://www.baidu.com'

#（2）模拟浏览器向服务器发送请求  response响应
response=urllib.request.urlopen(url)

###一个类型和六个方法###

#response是HTTPresponse的类型
#print(type(response))

#按照一个字节一个字节的去读
#content=response.read()
#print(content)

#返回多少个字节
#content=response.read(5)
#print(content)

#读取一行
#content=response.readline()
#print(content)

#content=response.readline()
#print(content)

#返回状态码  如果是200了 那么久证明我们的逻辑没有错
#print(response.getcode())

#返回的是url地址
#print(response.geturl())

#获取是一个状态信息
print(response.getheaders())

#一个类型 HTTPResponse
#六个方法 read readline readlines getcode geturl getheaders
```

```python
#_*_coding:utf-8_*_
#@Time:2022/10/1218:22
#@Auther:邻家小胡
#@File:获取网页源码_urllib
#@Project:untitled

#使用urllib来获取百度首页源码
import urllib.request

#（1）定义一个url   就是你要访问的地址
url='http://www.baidu.com'

#（2）模拟浏览器向服务器发送请求  response响应
reponse=urllib.request.urlopen(url)

#（3）获取响应中的页面的源码  content 内容的意思
#read方法  返回的是字节形式的二进制数据（带b,无中文）
#我们要将二进制数据转换为字符串
#二进制--》字符串  解码 decode('编码的格式')
content=reponse.read().decode('utf-8')

#（4）打印数据
print(content)


#ctrl+F  搜索一下有没有汉字
```

```python
#_*_coding:utf-8_*_
#@Time:2022/10/1220:41
#@Auther:邻家小胡
#@File:urllib_请求对象的定制
#@Project:untitled

import urllib.request
url='https://www.baidu.com'

#url的组成
#https://www.baidu.com/s?wd=周杰伦
#http/https  www.baidu.com   80/443   s     wd=周杰伦
#   协议           主机        端口号   路径     参数
#http    80
#https   443
#mysql   3306
#oracle  1521
#redis   6379
#mongodb  27017

headers={
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:105.0) Gecko/20100101 Firefox/105.0'
}

#因为urlopen方法方法中不能存储字典 所以headers不能传递出去
#请求对象的定制
#注意 因为参数顺序的问题   不能直接写url和headers 中间还有data 所以我们需要关键字传参
#伪装一下
request=urllib.request.Request(url=url,headers=headers)

response=urllib.request.urlopen(request)
content=response.read().decode('utf-8')
print(content)
```

# 2.第一个爬虫
```python
#_*_coding:utf-8_*_
#@Time:2022/10/1219:58
#@Auther:邻家小胡
#@File:爬虫_urllib_下载
#@Project:untitled

import urllib.request

#下载网页
#url_page='http://www.baidu.com'

#urllib.request.urlretrieve(url=url_page,filename='baidu.html')
#url代表的是下载路径   filename为文件的名字
#在python中 可以写变量的名字 也可以直接写值

#下载图片
#url_img='https://img0.baidu.com/it/u=2803685810,1196657358&fm=253&fmt=auto&app=120&f=JPEG?w=660&h=388'

#urllib.request.urlretrieve(url_img,'XuSong.jpg')

#下载视频
url_viedo='https://v26-web.douyinvod.com/3f9b1690b1ac341a76659d0fd0c48196/6346beea/video/tos/cn/tos-cn-ve-15c001-alinc2/5790ce78f4b04d2a97419929056a8217/?a=6383&ch=10&cr=3&dr=0&lr=all&cd=0|0|0|3&cv=1&br=1213&bt=1213&cs=0&ds=3&ft=X1nbLXvvBQa-ULZyd8Z.wNnOYZlcWAaSd2bLEh5I0DZm&mime_type=video_mp4&qs=0&rc=aGlpZTRnPGZlaTw8Zmk6ZkBpajdmZGY6ZndtZzMzNGkzM0AuYS5fMDM2XjIxNTQuNS0tYSNiaGVwcjRvci1gLS1kLWFzcw==&l=20221012201910010135168215041F2F56'

urllib.request.urlretrieve(url_viedo,'chengyooo.mp4')

#另一种下载视频方法
# url="https://v26-web.douyinvod.com/13d1db3121d78866998cc43cc570ca02/63453f91/video/tos/cn/tos-cn-ve-15c001-alinc2/b20ddadd909541fba8ccbf241c93fcf9/?a=6383&ch=10&cr=3&dr=0&lr=all&cd=0%7C0%7C0%7C3&cv=1&br=1574&bt=1574&cs=0&ds=3&ft=X1nbLXvvBQa-ULZyd8Z.wNnOYZlcPI8Sd2bLkrxS9iZm&mime_type=video_mp4&qs=0&rc=M2c2NzNnaGhmZDpkPDM3NkBpajxreWU6ZnhlZjMzNGkzM0BfX2IwMmIwXzIxNl9eYy9eYSNgaXJqcjRnLnNgLS1kLWFzcw%3D%3D&l=2022101117034601021209721833010F07"
#
# import requests
#
# res=requests.get(url)
# open('C:\\呵呵2.mp4','wb').write(res.content)
```

# 3.每日一练
```python

# import urllib.request as req
#
# #url="https://www.mn52.com/"
# #url="https://www.sina.com"
# #url="https://yz.chsi.com.cn/wap/"
# #url="https://pic.netbian.com/4kdongman/"
# #url="https://www.vmgirls.com/"
# url="https://www.github.com/"
#
# pageinfo=req.urlopen(url)
# print(pageinfo.read().decode("utf-8"))
#
# with open('github.txt','wb') as file:
#     file.write(pageinfo.read())




# # -*- coding: utf-8 -*-
# import urllib.request as req
# # 国防科技大学本科招生信息网中录取分数网页URL：
# url = 'http://www.gotonudt.cn/site/gfkdbkzsxxw/lqfs/index.html'  # 录取分数网页URL
# webpage = req.urlopen(url)  # 按照类文件的方式打开网页
# data = webpage.read()       # 一次性读取网页的所有数据
# data = data.decode('utf-8')  # 将byte类型的data解码为字符串（否则后面查找就要另外处理了）
# def step2():
# # 建立空列表urls，来保存子网页的url
#     urls = []
# # 请按下面的注释提示添加代码，完成相应功能
# #********** Begin *********#
# # 从data中提取2016到2012每一年分数线子网站地址添加到urls列表中
#     years=[2016,2015,2014,2013,2012]
#     for year in years:
#         index=data.find("国防科技大学%s年录取分数统计"%year)
#         href=data[index-80:index-39]  #根据单个特征串提取url子串
#         website='http://www.gotonudt.cn'
#         urls.append(website+href)
# #********** End **********#
#     return urls




# import urllib.request as req
# import requests
# import re
# # 国防科技大学本科招生信息网中录取分数网页URL：
# url = 'http://www.gotonudt.cn/site/gfkdbkzsxxw/lqfs/index.html'  # 录取分数网页URL
# webpage = req.urlopen(url)  # 按照类文件的方式打开网页
# data = webpage.read().decode('utf-8')       # 一次性读取网页的所有数据
#
# href=re.findall(r'<li class="flt"><a href="(.*?)"',data)  #正则匹配
# href=["http://www.gotonudt.cn"+link for link in href]   #拼接路径
# print(href)





# string = 'helloabcdefg'
# index = string.find("a")
# print(index)
# print(string[index-1])







# # -*- coding: utf-8 -*-
# import urllib.request as req
# import re
#
# # 国防科技大学本科招生信息网中2016年录取分数网页URL：
# url = 'http://www.gotonudt.cn/site/gfkdbkzsxxw/lqfs/info/2017/717.html'
# webpage = req.urlopen(url)      # 根据超链访问链接的网页
# data = webpage.read()           # 读取超链网页数据
# data = data.decode('utf-8')     # byte类型解码为字符串
# # 获取网页中的第一个表格中所有内容：
# table = re.findall(r'<table(.*?)</table>', data, re.S)
# firsttable = table[0]           # 取网页中的第一个表格
# # 数据清洗，将表中的&nbsp，\u3000，和空格号去掉
# firsttable = firsttable.replace('&nbsp;', '')
# firsttable = firsttable.replace('\u3000', '')
# firsttable = firsttable.replace(' ', '')
#
# def step3():
#     score = []
# # 请按下面的注释提示添加代码，完成相应功能，若要查看详细html代码，可在浏览器中打开url，查看页面源代码。
# #********** Begin *********#
# # 1.按tr标签对获取表格中所有行，保存在列表rows中：
#     rows=re.findall(r'tr(.*?)</tr>',firsttable,re.S)
# # 2.迭代rows中的所有元素，获取每一行的td标签内的数据，并把数据组成item列表，将每一个item添加到scorelist列表：
#     scorelist=[]
#     for row in rows:
#         items=[]
#         tds=re.findall(r'td(.*?)</td>',row,re.S)
#         for td in tds:
#             rightindex=td.find('</span>')#返回-1表示没有找到
#             leftindex=td[:rightindex].rfind('>')
#             items.append(td[leftindex+1:rightindex])
#         scorelist.append(items)
# # 3.将由省份，分数组成的8元列表（分数不存在的用/代替）作为元素保存到新列表score中，不要保存多余信息
#     for record in scorelist[3:]:
#         record.pop()
#         score.append(record)
# #********** End **********#
#   return score
```

# 4.爬取网络小说


[爬取网络小说.py](https://www.yuque.com/attachments/yuque/0/2024/py/39216292/1725700163461-5b8c72ad-d313-4f32-9794-f1e8bede3f0a.py)[爬虫网络小说保存文本.py](https://www.yuque.com/attachments/yuque/0/2024/py/39216292/1725699243085-f2b7938c-afd9-4117-981c-673a8b841d02.py)

[爬取网络小说完本.py](https://www.yuque.com/attachments/yuque/0/2024/py/39216292/1725699243063-6ba994ec-0c79-4a92-8579-f1d0e5f94f0a.py)[测试爬曲各平台网络小说.py](https://www.yuque.com/attachments/yuque/0/2024/py/39216292/1725700180505-8b8529ba-4a1b-41fb-9cb5-7e34d6902030.py)

```python
import requests
from bs4 import BeautifulSoup

if __name__ == "__main__":
    novel = "遮天.txt"
    url = "https://www.ddyueshu.com/1_1213/33776390.html"
    # 一定要带请求头，不然有些小说网站会获取不到
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    # 编码格式，防止中文乱码，有的是utf-8，有的是GBK，自己去试一下就好了
    # response.encoding = "utf-8"
    response.encoding = "GBK"
    # 解析html，获取小说
    bs = BeautifulSoup(response.text, "lxml")
    # 获取html节点
    texts = bs.find("div", id="content")
    print(texts.text)

```

![](https://cdn.nlark.com/yuque/0/2024/png/39216292/1725700224833-eb83b0da-36b0-4faf-a18f-dff2002c7d9f.png)

```python
import requests
from bs4 import BeautifulSoup


def writeNovel(filename, content):
    #单独的方法，方便复用
    f = open("D:\\" + filename, "a+", encoding="utf-8")
    f.write(content)
    f.flush()
    f.close()


if __name__ == "__main__":
    novel = "遮天.txt"
    url = "https://www.ddyueshu.com/1_1213/33776390.html"
    # 一定要带请求头，不然有些小说网站会获取不到
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    # 编码格式，防止中文乱码，有的是utf-8，有的是GBK，自己去试一下就好了，或者去看html的head标签里面也会有写
    # response.encoding = "utf-8"
    response.encoding = "GBK"
    # 解析html，获取小说
    bs = BeautifulSoup(response.text, "lxml")
    # 获取html节点
    texts = bs.find("div", id="content")
    # 清洗数据
    s = texts.text.replace("　　", "\n　　").replace(
        "请记住本书首发域名：ddyueshu.com。顶点小说手机版阅读网址：m.ddyueshu.com", "\n"
    )
    #保存小说
    writeNovel(novel, s)
    print(s)

```

![](https://cdn.nlark.com/yuque/0/2024/png/39216292/1725700087676-8073b3f4-dc0b-4d0f-9ac8-87adea3584bf.png)



```python
import time
import requests
from bs4 import BeautifulSoup


def writeNovel(filename, content):
	# a+为追加模式，指定编码为utf-8，防止很多gbk错误
    with open("D:\\" + filename, "a+", encoding="utf-8") as f:
        f.write(content)
        f.flush()


# 根据url获取章节内容
def getContent(url, title):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    # 编码格式，防止中文乱码，有的是utf-8，有的是GBK，自己去试一下就好了
    # response.encoding = "utf-8"
    response.encoding = "GBK"
    # 解析html，获取小说
    html = BeautifulSoup(response.text, "lxml")
    # 获取html节点
    texts = html.find("div", id="content")
    # 清洗数据
    content = texts.text.replace("　　", "\n　　").replace(
        "请记住本书首发域名：ddyueshu.com。顶点小说手机版阅读网址：m.ddyueshu.com", "\n"
    )
    return "\n" + title + "\n" + content


if __name__ == "__main__":
    start_time = time.time()
    novel = "牧神记.txt"
    # 小说域名地址
    server = "https://www.ddyueshu.com"
    # 小说目录地址
    contentsUrl = "https://www.ddyueshu.com/5_5189/"
    # 请求头
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36"
    }
    res = requests.get(contentsUrl, headers=headers)
    res.encoding = "GBK"
    # 解析html，获取小说
    html = BeautifulSoup(res.text, "lxml")
    # 获取html节点
    contentsList = html.find("div", id="list")
    # 获取所有a标签
    contents = contentsList.find_all("a")
    count = 1
    for item in contents:
        # 去掉重复的，从正文开始获取，根据实际情况灵活更改
        if count < 7:
            count += 1
            continue
        # 获取a标签内容
        title = item.text
        # 获取a标签地址链接
        url = server + item.get("href")
        print("开始下载 " + title)
        #获取小说内容
        content = getContent(url, title)
        # 写入文件
        writeNovel(novel, content)
    end_time = time.time()
    print("耗时: {:.2f}秒".format(end_time - start_time))
```

![](https://cdn.nlark.com/yuque/0/2024/png/39216292/1725699707621-c1029cd3-d069-4021-8a7c-7100458941df.png)

![](https://cdn.nlark.com/yuque/0/2024/png/39216292/1725700446890-4a11bd9c-f49f-4e7b-981c-c4ef680ae046.png)

![](https://cdn.nlark.com/yuque/0/2024/png/39216292/1725700473109-7d060c6b-3b4d-4856-a0bd-8afb42cd0cca.png)

[凡人修仙传之仙界篇（完本）.txt](https://www.yuque.com/attachments/yuque/0/2024/txt/39216292/1725700504813-a7a67130-34b5-4930-9c0a-c5012116e21e.txt) 

# 5.自定义网页
```html
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <title>Title</title>
  </head>
  <body>

    <!--  table  表格
    tr    行
    td    列
    -->
    <table width="200px" height="200px" border="1px">        <!--加表格线条框架参数-->
      <tr>
        <td>
          姓名
        </td>
        <td>
          年龄
        </td>
        <td>
          性别
        </td>
      </tr>

      <tr>
        <td>
          张三
        </td>
        <td>
          18
        </td>
        <td>
          男
        </td>
      </tr>
    </table>

    <!--         ul   li  无序列表  爬虫的使用场景非常之多-->
    <uL>
      <li>铁锅炖大鸭</li>
      <li>小鸡炖蘑菇</li>
      <li>锅包肉</li>
    </uL>

    <!--         ol   li  有序列表 -->
    <ol>
      <li>穿上衣服</li>
      <li>下床</li>
      <li>洗漱</li>
    </ol>

    <!--         网站  域名   -->
    <a href="https://omofun.tv/vod/show/by/score/id/20/page/1.html">邻家小胡的收藏网站</a>

  </body>
</html>
```

![](https://cdn.nlark.com/yuque/0/2024/png/39216292/1725189530414-38f44cea-ec27-47ee-a881-4df84312c83e.png)

